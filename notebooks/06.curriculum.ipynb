{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import subprocess\n",
    "import gc\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Notebook will only run for this amount of time\n",
    "time_start = time.time()\n",
    "time_elapsed = 0\n",
    "time_train_max = 8 * 60 * 60 # 8 hours"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        slice_depth: int = 65,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(slice_depth, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.LazyLinear(120)\n",
    "        self.fc2 = nn.LazyLinear(84)\n",
    "        self.fc3 = nn.LazyLinear(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle(img, threshold=0.5):\n",
    "    # TODO: Histogram of image to see where threshold should be\n",
    "    flat_img = img.flatten()\n",
    "    flat_img = np.where(flat_img > threshold, 1, 0).astype(np.uint8)\n",
    "    starts = np.array((flat_img[:-1] == 0) & (flat_img[1:] == 1))\n",
    "    ends = np.array((flat_img[:-1] == 1) & (flat_img[1:] == 0))\n",
    "    starts_ix = np.where(starts)[0] + 2\n",
    "    ends_ix = np.where(ends)[0] + 2\n",
    "    lengths = ends_ix - starts_ix\n",
    "    return starts_ix, lengths\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using GPU\")\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "def get_gpu_memory():\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.free', '--format=csv,nounits,noheader'], stdout=subprocess.PIPE, text=True)\n",
    "    gpu_memory = [tuple(map(int, line.split(','))) for line in result.stdout.strip().split('\\n')]\n",
    "    for i, (used, free) in enumerate(gpu_memory):\n",
    "        print(f\"GPU {i}: Memory Used: {used} MiB | Memory Available: {free} MiB\")\n",
    "        \n",
    "def clear_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print('Clearing GPU memory')\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurriculumDataset(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Directory containing the datasets\n",
    "        data_dirs: List[str],\n",
    "        # Expected slices per fragment\n",
    "        slice_depth: int = 65,\n",
    "        # Size of an individual patch\n",
    "        patch_size_x: int = 1028,\n",
    "        patch_size_y: int = 256,\n",
    "        # Image resize ratio\n",
    "        resize_ratio: float = 1.0,\n",
    "        # Training vs Testing mode\n",
    "        train: bool = True,\n",
    "        # Filenames of the images we'll use\n",
    "        image_mask_filename='mask.png',\n",
    "        image_labels_filename='inklabels.png',\n",
    "        slices_dir_filename='surface_volume',\n",
    "    ):\n",
    "        print(f\"Initializing CurriculumDataset\")\n",
    "        # Train mode also loads the labels\n",
    "        self.train = train\n",
    "        # Resize ratio reduces the size of the image\n",
    "        self.resize_ratio = resize_ratio\n",
    "        # Data will be B x slice_depth x patch_size_x x patch_size_y\n",
    "        self.patch_size_x = patch_size_x\n",
    "        self.patch_size_y = patch_size_y\n",
    "        self.slice_depth = slice_depth\n",
    "        # Potential N datasets\n",
    "        self.data_dirs = []\n",
    "        for data_dir in data_dirs:\n",
    "            assert os.path.exists(data_dir), f\"Data directory {data_dir} does not exist\"\n",
    "            self.data_dirs.append(data_dir)\n",
    "            # Open Mask image\n",
    "            _image_mask_filepath = os.path.join(data_dir, image_mask_filename)\n",
    "            _mask_img = Image.open(_image_mask_filepath).convert(\"1\")\n",
    "            # Get original size and resized size\n",
    "            original_size = _mask_img.size\n",
    "            resized_size = (\n",
    "                int(original_size[0] * self.resize_ratio),\n",
    "                int(original_size[1] * self.resize_ratio),\n",
    "            )\n",
    "            # Resize the mask\n",
    "            print(f\"Mask original size: {original_size}\")\n",
    "            _mask_img = _mask_img.resize(resized_size, resample=Image.BILINEAR)\n",
    "            print(f\"Mask resized size: {_mask_img.size}\")\n",
    "            _mask = torch.from_numpy(np.array(_mask_img)).to(torch.bool)\n",
    "            print(f\"Mask tensor shape: {_mask.shape}\")\n",
    "            print(f\"Mask tensor dtype: {_mask.dtype}\")\n",
    "            if train:\n",
    "                _image_labels_filepath = os.path.join(data_dir, image_labels_filename)\n",
    "                _labels_img = Image.open(_image_labels_filepath).convert(\"1\")\n",
    "                print(f\"Labels original size: {original_size}\")\n",
    "                _labels_img = _labels_img.resize(resized_size, resample=Image.BILINEAR)\n",
    "                print(f\"Labels resized size: {_labels_img.size}\")\n",
    "                _labels = torch.from_numpy(np.array(_labels_img)).to(torch.bool)\n",
    "                print(f\"Labels tensor shape: {_labels.shape}\")\n",
    "                print(f\"Labels tensor dtype: {_labels.dtype}\")\n",
    "            # Pre-allocate the entire fragment\n",
    "            _fragment = torch.zeros((\n",
    "                    self.slice_depth,\n",
    "                    resized_size[1],\n",
    "                    resized_size[0],\n",
    "                ), dtype=torch.float32\n",
    "            )\n",
    "            print(f\"Fragment tensor shape: {_fragment.shape}\")\n",
    "            print(f\"Fragment tensor dtype: {_fragment.dtype}\")\n",
    "            # Open up slices\n",
    "            _slice_dir = os.path.join(data_dir, slices_dir_filename)\n",
    "            for i in tqdm(range(self.slice_depth)):\n",
    "                _slice_filepath = os.path.join(_slice_dir, f\"{i:02d}.tif\")\n",
    "                _slice_img = Image.open(_slice_filepath).convert('F')\n",
    "                print(f\"Slice original size: {original_size}\")\n",
    "                _slice_img = _slice_img.resize(resized_size, resample=Image.BILINEAR)\n",
    "                print(f\"Slice resized size: {_slice_img.size}\")\n",
    "                _slice = torch.from_numpy(np.array(_slice_img)/65535.0)\n",
    "                print(f\"Slice tensor shape: {_slice.shape}\")\n",
    "                print(f\"Slice tensor dtype: {_slice.dtype}\")\n",
    "                _fragment[i, :, :] = _slice\n",
    "\n",
    "            print(f\"Fragment tensor shape: {_fragment.shape}\")\n",
    "            print(f\"Fragment tensor dtype: {_fragment.dtype}\")\n",
    "            print(f\"Fragment tensor min: {_fragment.min()}\")\n",
    "            print(f\"Fragment tensor max: {_fragment.max()}\")\n",
    "            print(f\"Fragment tensor mean: {_fragment.mean()}\")\n",
    "            print(f\"Fragment tensor std: {_fragment.std()}\")\n",
    "\n",
    "            # Normalize the fragment only on mask indices\n",
    "            _mask_indices = torch.nonzero(_mask)\n",
    "            _masked_fragment = _fragment[_mask]\n",
    "            _mean = torch.mean(_masked_fragment)\n",
    "            _std = torch.std(_masked_fragment)\n",
    "\n",
    "        # Pad the fragment to make sure we can get patches from the edges\n",
    "        self.fragment = torch.nn.functional.pad(\n",
    "            self.fragment, pad_sizes, mode='constant', value=0.0)\n",
    "\n",
    "        # Get indices where mask is 1\n",
    "        self.mask_indices = torch.nonzero(self.mask.squeeze())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.mask_indices.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the x, y from the mask indices\n",
    "        x, y = self.mask_indices[index]\n",
    "\n",
    "        # Get the patch\n",
    "        patch = self.fragment[\n",
    "            :,\n",
    "            x: x + self.patch_size_x,\n",
    "            y: y + self.patch_size_y,\n",
    "        ]\n",
    "\n",
    "        # Label is going to be the label of the center voxel\n",
    "        if self.train:\n",
    "            label = self.labels[\n",
    "                0,\n",
    "                x,\n",
    "                y,\n",
    "            ]\n",
    "\n",
    "        # Normalize the patch based on dataset\n",
    "        patch = (patch - self.mean) / self.std\n",
    "\n",
    "        if self.train:\n",
    "            return patch, label\n",
    "        else:\n",
    "            # If we're not training, we don't have labels\n",
    "            return patch\n",
    "\n",
    "train_dir = '/home/tren/dev/ashenvenus/data/train/1'\n",
    "train_dataset = CurriculumDataset(\n",
    "    # Directory containing the dataset\n",
    "    data_dirs = [\n",
    "        train_dir,\n",
    "    ],\n",
    "    # Expected slices per fragment\n",
    "    slice_depth=65,\n",
    "    # Size of an individual patch\n",
    "    patch_size_x=256,\n",
    "    patch_size_y=64,\n",
    "    # Image resize ratio\n",
    "    resize_ratio=0.1,\n",
    "    # Training vs Testing mode\n",
    "    train=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dir = '/kaggle/input/vesuvius-challenge-ink-detection/train/1'\n",
    "# eval_dir = '/kaggle/input/vesuvius-challenge-ink-detection/test'\n",
    "train_dir = '/home/tren/dev/ashenvenus/data/train/1'\n",
    "eval_dir = '/home/tren/dev/ashenvenus/data/test'\n",
    "slice_depth = 65\n",
    "patch_size_x = 256\n",
    "patch_size_y = 64\n",
    "resize_ratio = 0.25\n",
    "batch_size = 512\n",
    "num_workers = 1\n",
    "lr = 0.0002\n",
    "num_epochs = 2\n",
    "threshold = 0.5\n",
    "train_dataset_size = 1000\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "model = None\n",
    "test_batch = None\n",
    "\n",
    "print(\"START\")\n",
    "clear_gpu_memory()\n",
    "get_gpu_memory()\n",
    "\n",
    "# Load the model, try to fit on GPU\n",
    "model = SimpleNet(\n",
    "    slice_depth=slice_depth,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"LOADED MODEL\")\n",
    "get_gpu_memory()\n",
    "\n",
    "# # Empty array to test batch size\n",
    "# for i in range(10):\n",
    "#     test_batch = torch.zeros(batch_size, slice_depth, patch_size_x, patch_size_y)\n",
    "#     test_batch = test_batch.to(device)\n",
    "#     print(f\"LOADED BATCH {i}\")\n",
    "#     get_gpu_memory()\n",
    "#     with torch.no_grad():\n",
    "#         model(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "train_dataset = ClassificationDataset(\n",
    "    # Directory containing the dataset\n",
    "    train_dir,\n",
    "    # Expected slices per fragment\n",
    "    slice_depth=slice_depth,\n",
    "    # Size of an individual patch\n",
    "    patch_size_x=patch_size_x,\n",
    "    patch_size_y=patch_size_y,\n",
    "    # Image resize ratio\n",
    "    resize_ratio=resize_ratio,\n",
    "    # Training vs Testing mode\n",
    "    train=True,\n",
    ")\n",
    "total_dataset_size = len(train_dataset)\n",
    "\n",
    "print(\"LOADED DATASET\")\n",
    "get_gpu_memory()\n",
    "\n",
    "train_idx = [i for i in range(total_dataset_size)]\n",
    "print(f\"Raw train dataset size: {len(train_idx)}\")\n",
    "np.random.shuffle(train_idx)\n",
    "train_idx = train_idx[:train_dataset_size]\n",
    "print(f\"Reduced train dataset size: {len(train_idx)}\")\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    # Shuffle does NOT work\n",
    "    shuffle=False,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=num_workers,\n",
    "    # This will make it go faster if it is loaded into a GPU\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Create optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Train the model\n",
    "best_valid_loss = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1} of {num_epochs}\")\n",
    "\n",
    "    print(f\"Training...\")\n",
    "    train_loss = 0\n",
    "    for patch, label in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        patch = patch.to(device)\n",
    "        label = label.to(device).unsqueeze(1).to(torch.float32)\n",
    "\n",
    "        print(f\"TRAINING ON BATCH\")\n",
    "        get_gpu_memory()\n",
    "\n",
    "        pred = model(patch)\n",
    "        loss = loss_fn(pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()  # Accumulate the training loss\n",
    "\n",
    "        time_elapsed = time.time() - time_start\n",
    "        if time_elapsed > time_train_max:\n",
    "            print('Time limit reached, stopping batch')\n",
    "            break\n",
    "\n",
    "    if time_elapsed > time_train_max:\n",
    "        print('Time limit reached, stopping epoch')\n",
    "        break\n",
    "\n",
    "    # Calculate the average training loss\n",
    "    train_loss /= len(train_dataloader)\n",
    "    print(f\"Training loss: {train_loss:.4f}\")\n",
    "\n",
    "del train_dataloader, train_dataset, train_sampler\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Create submission file\n",
    "submission_filepath = 'submission.csv'\n",
    "with open(submission_filepath, 'w') as f:\n",
    "    # Write header\n",
    "    f.write(\"Id,Predicted\\n\")\n",
    "\n",
    "# Baseline is to use image mask to create guess submission\n",
    "for subtest_name in os.listdir(eval_dir):\n",
    "\n",
    "    # Name of sub-directory inside test dir\n",
    "    subtest_filepath = os.path.join(eval_dir, subtest_name)\n",
    "    # Evaluation dataset\n",
    "    eval_dataset = ClassificationDataset(\n",
    "        # Directory containing the dataset\n",
    "        subtest_filepath,\n",
    "        # Expected slices per fragment\n",
    "        slice_depth=slice_depth,\n",
    "        # Size of an individual patch\n",
    "        patch_size_x=patch_size_x,\n",
    "        patch_size_y=patch_size_y,\n",
    "        # Image resize ratio\n",
    "        resize_ratio=resize_ratio,\n",
    "        # Training vs Testing mode\n",
    "        train=False,\n",
    "    )\n",
    "\n",
    "    # Make a blank prediction image\n",
    "    pred_image = np.zeros(eval_dataset.mask.shape[1:], dtype=np.uint8)\n",
    "\n",
    "    # DataLoaders\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=1,\n",
    "        # Shuffle does NOT work\n",
    "        shuffle=False,\n",
    "        sampler=SequentialSampler(eval_dataset),\n",
    "        num_workers=num_workers,\n",
    "        # This will make it go faster if it is loaded into a GPU\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    for i, patch in enumerate(tqdm(eval_dataloader)):\n",
    "        patch = patch.to(device)\n",
    "        pixel_index = eval_dataset.mask_indices[i]\n",
    "        with torch.no_grad():\n",
    "            pred = model(patch)\n",
    "            pred = torch.sigmoid(pred)\n",
    "        if pred > threshold:\n",
    "            pred_image[pixel_index[0], pixel_index[1]] = 1\n",
    "            \n",
    "    # Resize pred_image to original size\n",
    "    _img = Image.fromarray(pred_image)\n",
    "    _img = _img.resize((\n",
    "        eval_dataset.original_image_size_x,\n",
    "        eval_dataset.original_image_size_y,\n",
    "    ))\n",
    "    pred_image = np.array(_img)\n",
    "    \n",
    "        \n",
    "    starts_ix, lengths = rle(pred_image)\n",
    "    inklabels_rle = \" \".join(map(str, sum(zip(starts_ix, lengths), ())))\n",
    "    with open(submission_filepath, 'a') as f:\n",
    "        f.write(f\"{subtest_name},{inklabels_rle}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ashenvenus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
