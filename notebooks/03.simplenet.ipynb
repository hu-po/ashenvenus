{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        slice_depth: int = 65,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(slice_depth, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.LazyLinear(120)\n",
    "        self.fc2 = nn.LazyLinear(84)\n",
    "        self.fc3 = nn.LazyLinear(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle(img, threshold=0.5):\n",
    "    # TODO: Histogram of image to see where threshold should be\n",
    "    flat_img = img.flatten()\n",
    "    flat_img = np.where(flat_img > threshold, 1, 0).astype(np.uint8)\n",
    "    starts = np.array((flat_img[:-1] == 0) & (flat_img[1:] == 1))\n",
    "    ends = np.array((flat_img[:-1] == 1) & (flat_img[1:] == 0))\n",
    "    starts_ix = np.where(starts)[0] + 2\n",
    "    ends_ix = np.where(ends)[0] + 2\n",
    "    lengths = ends_ix - starts_ix\n",
    "    return starts_ix, lengths\n",
    "\n",
    "def load_image(\n",
    "    image_filepath: str = 'image.png',\n",
    "    image_type: str = None,\n",
    "    tensor_dtype: torch.dtype = torch.float32,\n",
    "    resize_ratio: float = 1.0,\n",
    "    viz: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    _image = Image.open(image_filepath)\n",
    "    if image_type is not None:\n",
    "        # ‘L’ (8-bit pixels, grayscale)\n",
    "        # ‘F’ (32-bit floating point pixels)\n",
    "        log.debug(f\"Converting {image_filepath} to {image_type}\")\n",
    "        _image = _image.convert(image_type)\n",
    "    if resize_ratio != 1.0:\n",
    "        log.debug(f\"Resizing {image_filepath} by {resize_ratio}\")\n",
    "        _image = _image.resize((\n",
    "            int(_image.width * resize_ratio),\n",
    "            int(_image.height * resize_ratio)\n",
    "        ), resample=Image.BILINEAR)\n",
    "    _pt = ToTensor()(_image)\n",
    "    _pt = _pt.to(tensor_dtype)\n",
    "    return _pt\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        log.info(\"Using GPU\")\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        log.info(\"Using CPU\")\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Directory containing the dataset\n",
    "        data_dir: str,\n",
    "        # Filenames of the images we'll use\n",
    "        image_mask_filename='mask.png',\n",
    "        image_labels_filename='inklabels.png',\n",
    "        image_ir_filename='ir.png',\n",
    "        slices_dir_filename='surface_volume',\n",
    "        # Expected slices per fragment\n",
    "        slice_depth: int = 65,\n",
    "        # Size of an individual patch\n",
    "        patch_size_x: int = 1028,\n",
    "        patch_size_y: int = 256,\n",
    "        # Image resize ratio\n",
    "        resize_ratio: float = 1.0,\n",
    "        # Dataset datatype\n",
    "        dataset_dtype: torch.dtype = torch.float32,\n",
    "        # Training vs Testing mode\n",
    "        train: bool = True,\n",
    "        # Visualize the dataset\n",
    "        viz: bool = False,\n",
    "    ):\n",
    "        self.train = train\n",
    "        self.viz = viz\n",
    "        log.info(f\"Initializing FragmentDataset with data_dir={data_dir}\")\n",
    "        # Verify paths and directories for images and metadata\n",
    "        self.data_dir = data_dir\n",
    "        assert os.path.exists(\n",
    "            data_dir), f\"Data directory {data_dir} does not exist\"\n",
    "        self.image_mask_filename = image_mask_filename\n",
    "        self.image_mask_filepath = os.path.join(\n",
    "            data_dir, self.image_mask_filename)\n",
    "        assert os.path.exists(\n",
    "            self.image_mask_filepath), f\"Mask file {self.image_mask_filepath} does not exist\"\n",
    "        self.image_labels_filename = image_labels_filename\n",
    "        self.image_labels_filepath = os.path.join(\n",
    "            data_dir, self.image_labels_filename)\n",
    "        self.slices_dir = os.path.join(data_dir, slices_dir_filename)\n",
    "        assert os.path.exists(\n",
    "            self.slices_dir), f\"Slices directory {self.slices_dir} does not exist\"\n",
    "        if self.train:\n",
    "            assert os.path.exists(\n",
    "                self.image_labels_filepath), f\"Labels file {self.image_labels_filepath} does not exist\"\n",
    "            self.image_ir_filename = image_ir_filename\n",
    "            self.image_ir_filepath = os.path.join(\n",
    "                data_dir, self.image_ir_filename)\n",
    "            assert os.path.exists(\n",
    "                self.image_ir_filepath), f\"IR file {self.image_ir_filepath} does not exist\"\n",
    "\n",
    "        # Resize ratio reduces the size of the image\n",
    "        self.resize_ratio = resize_ratio\n",
    "\n",
    "        # Load the meta data (mask, labels, and IR images)\n",
    "        self.mask = load_image(\n",
    "            self.image_mask_filepath,\n",
    "            image_type='L',\n",
    "            tensor_dtype=torch.bool,\n",
    "            resize_ratio=self.resize_ratio,\n",
    "            viz=self.viz,\n",
    "        )\n",
    "        if self.train:\n",
    "            self.labels = load_image(\n",
    "                self.image_labels_filepath,\n",
    "                image_type='L',\n",
    "                tensor_dtype=torch.bool,\n",
    "                resize_ratio=self.resize_ratio,\n",
    "                viz=self.viz,\n",
    "            )\n",
    "\n",
    "        # Assert that there are the correct amount of slices\n",
    "        self.slice_depth = slice_depth\n",
    "\n",
    "        # Dataset type determines precision of the data\n",
    "        self.dataset_dtype = dataset_dtype\n",
    "\n",
    "        # Load a single slice to get the width and height\n",
    "        _slice = load_image(\n",
    "            os.path.join(self.slices_dir, '00.tif'),\n",
    "            image_type='F',\n",
    "            tensor_dtype=dataset_dtype,\n",
    "            resize_ratio=self.resize_ratio,\n",
    "            viz=self.viz\n",
    "        )\n",
    "        self.fragment_size_x = _slice.shape[1]\n",
    "        self.fragment_size_y = _slice.shape[2]\n",
    "\n",
    "        # Load the slices (tif files) into one tensor, pre-allocate\n",
    "        self.fragment = torch.zeros(\n",
    "            self.slice_depth,\n",
    "            self.fragment_size_x,\n",
    "            self.fragment_size_y,\n",
    "            dtype=self.dataset_dtype,\n",
    "        )\n",
    "        for i in tqdm(range(self.slice_depth)):\n",
    "            _slice = load_image(\n",
    "                os.path.join(self.slices_dir, f\"{i:02d}.tif\"),\n",
    "                image_type='F',\n",
    "                tensor_dtype=dataset_dtype,\n",
    "                resize_ratio=self.resize_ratio,\n",
    "            )\n",
    "            self.fragment[i, :, :] = _slice\n",
    "            \n",
    "        # Get the mean and std of the fragment at the mask indices\n",
    "        masked_fragment = torch.masked_select(self.fragment, self.mask)\n",
    "        self.mean = torch.mean(masked_fragment, dim=-1)\n",
    "        self.std = torch.std(masked_fragment, dim=-1)\n",
    "\n",
    "        # Make sure the patch sizes are valid\n",
    "        self.patch_size_x = patch_size_x\n",
    "        self.patch_size_y = patch_size_y\n",
    "\n",
    "        # Store the half-sizes of the patches\n",
    "        self.patch_half_size_x = self.patch_size_x // 2\n",
    "        self.patch_half_size_y = self.patch_size_y // 2\n",
    "        pad_sizes = (\n",
    "            # Padding in Y\n",
    "            self.patch_half_size_y, self.patch_half_size_y,\n",
    "            # Padding in X\n",
    "            self.patch_half_size_x, self.patch_half_size_x,\n",
    "            # No padding on z\n",
    "            0, 0,\n",
    "        )\n",
    "\n",
    "        # Pad the fragment to make sure we can get patches from the edges\n",
    "        self.fragment = torch.nn.functional.pad(\n",
    "            self.fragment, pad_sizes, mode='constant', value=0.0)\n",
    "\n",
    "        # Get indices where mask is 1\n",
    "        self.mask_indices = torch.nonzero(self.mask.squeeze())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.mask_indices.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the x, y from the mask indices\n",
    "        x, y = self.mask_indices[index]\n",
    "\n",
    "        # Get the patch\n",
    "        patch = self.fragment[\n",
    "            :,\n",
    "            x: x + self.patch_size_x,\n",
    "            y: y + self.patch_size_y,\n",
    "        ]\n",
    "\n",
    "        # Label is going to be the label of the center voxel\n",
    "        if self.train:\n",
    "            label = self.labels[\n",
    "                0,\n",
    "                x,\n",
    "                y,\n",
    "            ]\n",
    "\n",
    "        # Normalize the patch based on dataset\n",
    "        patch = (patch - self.mean) / self.std\n",
    "\n",
    "        if self.train:\n",
    "            return patch, label\n",
    "        else:\n",
    "            # If we're not training, we don't have labels\n",
    "            return patch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/kaggle/input/vesuvius-challenge-ink-detection/train/1'\n",
    "eval_dir = '/kaggle/input/vesuvius-challenge-ink-detection/test'\n",
    "slice_depth = 65\n",
    "patch_size_x = 64\n",
    "patch_size_y = 64\n",
    "resize_ratio = 0.25\n",
    "batch_size = 128\n",
    "num_workers = 32\n",
    "lr = 0.01\n",
    "num_epochs = 2\n",
    "threshold = 0.5\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# Load the model, try to fit on GPU\n",
    "model = SimpleNet(\n",
    "    slice_depth=slice_depth,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Training dataset\n",
    "train_dataset = ClassificationDataset(\n",
    "    # Directory containing the dataset\n",
    "    train_dir,\n",
    "    # Expected slices per fragment\n",
    "    slice_depth=slice_depth,\n",
    "    # Size of an individual patch\n",
    "    patch_size_x=patch_size_x,\n",
    "    patch_size_y=patch_size_y,\n",
    "    # Image resize ratio\n",
    "    resize_ratio=resize_ratio,\n",
    "    # Training vs Testing mode\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "# Sampler for Train and Validation\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    # Shuffle does NOT work\n",
    "    shuffle=False,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=num_workers,\n",
    "    # This will make it go faster if it is loaded into a GPU\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Create optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Train the model\n",
    "best_valid_loss = 0\n",
    "for epoch in range(num_epochs):\n",
    "    log.info(f\"Epoch {epoch + 1} of {num_epochs}\")\n",
    "\n",
    "    log.info(f\"Training...\")\n",
    "    train_loss = 0\n",
    "    for patch, label in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        patch = patch.to(device)\n",
    "        label = label.to(device).unsqueeze(1).to(torch.float32)\n",
    "        pred = model(patch)\n",
    "        loss = loss_fn(pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()  # Accumulate the training loss\n",
    "\n",
    "    # Calculate the average training loss\n",
    "    train_loss /= len(train_dataloader)\n",
    "    log.info(f\"Training loss: {train_loss:.4f}\")\n",
    "\n",
    "del train_dataloader, train_dataset, train_sampler\n",
    "\n",
    "# Create submission file\n",
    "submission_filepath = 'submission.csv'\n",
    "with open(submission_filepath, 'w') as f:\n",
    "    # Write header\n",
    "    f.write(\"Id,Predicted\\n\")\n",
    "\n",
    "# Baseline is to use image mask to create guess submission\n",
    "for subtest_name in os.listdir(eval_dir):\n",
    "\n",
    "    # Name of sub-directory inside test dir\n",
    "    subtest_filepath = os.path.join(eval_dir, subtest_name)\n",
    "    # Evaluation dataset\n",
    "    eval_dataset = ClassificationDataset(\n",
    "        # Directory containing the dataset\n",
    "        subtest_filepath,\n",
    "        # Expected slices per fragment\n",
    "        slice_depth=slice_depth,\n",
    "        # Size of an individual patch\n",
    "        patch_size_x=patch_size_x,\n",
    "        patch_size_y=patch_size_y,\n",
    "        # Image resize ratio\n",
    "        resize_ratio=resize_ratio,\n",
    "        # Training vs Testing mode\n",
    "        train=False,\n",
    "    )\n",
    "\n",
    "    # Make a blank prediction image\n",
    "    pred_image = np.zeros(eval_dataset.mask.shape[1:], dtype=np.uint8)\n",
    "\n",
    "    # DataLoaders\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=1,\n",
    "        # Shuffle does NOT work\n",
    "        shuffle=False,\n",
    "        sampler=SequentialSampler(eval_dataset),\n",
    "        num_workers=num_workers,\n",
    "        # This will make it go faster if it is loaded into a GPU\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    for i, patch in enumerate(tqdm(eval_dataloader)):\n",
    "        patch = patch.to(device)\n",
    "        pixel_index = eval_dataset.mask_indices[i]\n",
    "        with torch.no_grad():\n",
    "            pred = model(patch)\n",
    "            pred = torch.sigmoid(pred)\n",
    "        if pred > threshold:\n",
    "            pred_image[pixel_index[0], pixel_index[1]] = 1\n",
    "        \n",
    "    starts_ix, lengths = rle(pred_image)\n",
    "    inklabels_rle = \" \".join(map(str, sum(zip(starts_ix, lengths), ())))\n",
    "    with open(submission_filepath, 'a') as f:\n",
    "        f.write(f\"{subtest_name},{inklabels_rle}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
